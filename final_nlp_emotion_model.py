# -*- coding: utf-8 -*-
"""final_nlp_emotion_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D3mBcboi2bznUtkb39XrCrf3P9fM7XY6

Install requirements and import dataset
"""

pip install torch transformers datasets scikit-learn faiss-cpu pandas tqdm

from datasets import load_dataset

dataset = load_dataset("go_emotions")
print(dataset["train"][0])

from transformers import AutoTokenizer
from datasets import Sequence, Value

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
label_names = dataset['train'].features['labels'].feature.names

num_labels = len(label_names)



def tokenize_and_format(example):
    tokens = tokenizer(example["text"], truncation=True, padding="max_length", max_length=128)

    label_vec = [0.0] * num_labels
    for idx in example["labels"]:
          label_vec[idx] = 1.0

    tokens["labels"] = list(map(float, label_vec))
    return tokens


encoded_dataset = dataset.map(tokenize_and_format, batched=False)


encoded_dataset = encoded_dataset.cast_column("labels", Sequence(Value("float32")))


print(encoded_dataset["train"][0]["labels"])
print(type(encoded_dataset["train"][0]["labels"][0]))

"""Finetuning distill bert"""

from sklearn.metrics import f1_score, precision_score, recall_score
import numpy as np
import torch

def compute_metrics(pred):
    logits, labels = pred
    probs = torch.sigmoid(torch.tensor(logits)).numpy()
    preds = (probs >= 0.5).astype(int)

    return {
        "micro/f1": f1_score(labels, preds, average="micro", zero_division=0),
        "macro/f1": f1_score(labels, preds, average="macro", zero_division=0),
        "micro/precision": precision_score(labels, preds, average="micro", zero_division=0),
        "micro/recall": recall_score(labels, preds, average="micro", zero_division=0),
    }

import torch
from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

model_name = "bert-base-uncased"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=num_labels,
    problem_type="multi_label_classification"
)



training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    num_train_epochs=8,
)


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset["validation"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()

"""Validate"""

from sklearn.metrics import classification_report
import torch
import numpy as np

def evaluate_on_dataset(trainer, dataset, label_names, threshold=0.5):
    preds_output = trainer.predict(dataset)
    probs = torch.sigmoid(torch.tensor(preds_output.predictions)).numpy()
    preds = (probs >= threshold).astype(int)
    labels = preds_output.label_ids

    print("Classification Report:")
    print(classification_report(labels, preds, target_names=label_names, zero_division=0))

evaluate_on_dataset(trainer, encoded_dataset["validation"], label_names, threshold=0.3)

import torch

  def predict_emotions(trainer, text, tokenizer, label_names, threshold=0.3):
      inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)

      device = trainer.model.device
      inputs = {key: value.to(device) for key, value in inputs.items()}
      with torch.no_grad():
          logits = trainer.model(**inputs).logits
      probs = torch.sigmoid(logits).squeeze().cpu().numpy()
      preds = (probs >= threshold).astype(int)
      print(f"Text: {text}")
      print("Label Predictions:")
      for label_name, probability, pred in zip(label_names, probs, preds):
          print(f"  â€¢ {label_name}: probability={probability:.3f}, predicted={pred}")

      return probs, preds


  input_txt = "She broke up with me."
  probs, preds = predict_emotions(trainer, input_txt, tokenizer, label_names)
  print(probs)
  print(preds)

trainer.save_model("./my_saved_model")

from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

# Load fine-tuned model
model_path = "./my_saved_model"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)

# Emotion classification pipeline
emotion_pipeline = pipeline("text-classification", model=model, tokenizer=tokenizer, return_all_scores=True)

import pandas as pd
import numpy as np
import faiss
import json
from tqdm import tqdm
from sklearn.preprocessing import normalize

df = pd.read_csv("spotify_millsongdata.csv")

def get_emotion_vector(lyrics):
    try:
        result = emotion_pipeline(lyrics[:512])
        return [s['score'] for s in result[0]]
    except:
        return [0.0] * model.config.num_labels

tqdm.pandas()
df['emotion_vector'] = df['text'].progress_apply(get_emotion_vector)
df['emotion_vector'] = df['emotion_vector'].apply(lambda x: normalize([x])[0])
emotion_matrix = np.vstack(df['emotion_vector'].values).astype('float32')
index = faiss.IndexFlatL2(emotion_matrix.shape[1])
index.add(emotion_matrix)
metadata = df[['artist', 'song', 'link']].to_dict('records')
faiss.write_index(index, "emotion_index.faiss")
with open("emotion_metadata.jsonl", "w") as f:
    for record in metadata:
        f.write(json.dumps(record) + "\n")

index = faiss.read_index("emotion_index.faiss")
with open("emotion_metadata.jsonl", "r") as f:
    metadata = [json.loads(line) for line in f]

def get_user_emotion_vector(text):
    result = emotion_pipeline(text[:512])
    vec = np.array([s['score'] for s in result[0]], dtype=np.float32)
    return normalize([vec])[0]

def recommend_songs_from_emotion(input_text, k):
    user_vec = get_user_emotion_vector(input_text)
    D, I = index.search(np.array([user_vec], dtype='float32'), k)
    return [metadata[i] for i in I[0]]

from operator import ge
input_text = "I am feeling lonely and sad"
print("Input text: ", input_text)
print('User emotion vector: ', get_user_emotion_vector(input_text))
top_k_songs = recommend_songs_from_emotion(input_text, k=5) #top 5
print("Model recommendations:")
for song in top_k_songs:
    print(f"{song['artist']} - {song['song']}")

#baseline cos sim
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

tfidf = TfidfVectorizer(stop_words='english', max_features=10000)
tfidf_matrix = tfidf.fit_transform(df['text'])

def recommend_tfidf_lyrics(input_text, k):
    input_vec = tfidf.transform([input_text])
    similarities = cosine_similarity(input_vec, tfidf_matrix).flatten()
    top_indices = similarities.argsort()[::-1][:k]
    return df.iloc[top_indices][['artist', 'song', 'link']].to_dict('records')

# baseline output
user_input = "I am feeling lonely and sad"
print("Input text: ", input_text)
print('User emotion vector: ', get_user_emotion_vector(input_text))
tfidf_results = recommend_tfidf_lyrics(user_input, k=5)
print("TF-IDF recommendations:")
for song in tfidf_results:
    print(f"{song['artist']} - {song['song']}")